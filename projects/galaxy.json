[
  {
    "id": "duan2024diffretouch",
    "title": "DiffRetouch: Using Diffusion to Retouch on the Shoulder of Experts",
    "authors": [
      "Zheng-Peng Duan",
      "Zheng Lin",
      "Xin Jin",
      "Dongqing Zou",
      "Chunle Guo",
      "Chongyi Li"
    ],
    "abstract": "Image retouching aims to enhance the visual quality of photos. Considering the different aesthetic preferences of users, the target of retouching is subjective. However, current retouching methods mostly adopt deterministic models, which not only neglects the style diversity in the expert-retouched results and tends to learn an average style during training, but also lacks sample diversity during inference. In this paper, we propose a diffusion-based method, named DiffRetouch. Thanks to the excellent distribution modeling ability of diffusion, our method can capture the complex fine-retouched distribution covering various visual-pleasing styles in the training data. Moreover, four image attributes are made adjustable to provide a user-friendly editing mechanism. By adjusting these attributes in specified ranges, users are allowed to customize preferred styles within the learned fine-retouched distribution. Additionally, the affine bilateral grid and contrastive learning scheme are introduced to handle the problem of texture distortion and control insensitivity respectively. Extensive experiments have demonstrated the superior performance of our method on visually appealing and sample diversity. The code will be made available to the community.",
    "url": "https://arxiv.org/abs/2407.03757",
    "tags": [
      "Diffusion",
      "Retouching"
    ],
    "homepage": "https://adam-duan.github.io/projects/retouch/",
    "published": "AAAI 2024",
    "teaser": "<img src='/assets/paper_teaser/diffretouch_teaser.png' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Adam-duan/DiffRetouch",
    "core_contributor": false,
    "pos": [
      2.3041999340057373,
      10.7701997756958,
      0.5425999760627747
    ],
    "stars": 22
  },
  {
    "id": "duan2025dit4sr",
    "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution",
    "authors": [
      "Zheng-Peng Duan",
      "Jiawei Zhang",
      "Xin Jin",
      "Ziheng Zhang",
      "Zheng Xiong",
      "Dongqing Zou",
      "Jimmy Ren",
      "Chunle Guo",
      "Chongyi Li"
    ],
    "abstract": "Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiT's limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments.",
    "url": "https://arxiv.org/abs/2503.23580",
    "tags": [
      "Diffusion",
      "Super-Resolution"
    ],
    "homepage": "https://adam-duan.github.io/projects/dit4sr/",
    "published": "ICCV 2025",
    "teaser": "<img src='/assets/paper_teaser/dit4sr/dit4sr.gif' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Adam-duan/DiT4SR",
    "core_contributor": false,
    "pos": [
      2.0436999797821045,
      9.821200370788574,
      1.3193999528884888
    ],
    "stars": 197
  },
  {
    "id": "guo2022novel",
    "title": "A Novel Low-light Image Enhancement Algorithm Based On Information Assistance",
    "authors": [
      "Jiacen Guo",
      "Xin Jin",
      "Weilin Chen",
      "Chao Wang"
    ],
    "abstract": "Low-light image enhancement plays a significant role in image processing and analysis. Nevertheless, most current algorithms are suffering from the problem of color distortion. With the assistance of an image captured in normal illumination as a reference, the matter of color distortion can be effectively settled by means of the color transfer method. The enhanced images in our proposed algorithm are much closer to the color distribution of the original objects under normal illumination. Our proposed algorithm takes a low-light image and a reference image with a different viewpoint or similar scene as input. Two images are both divided into regions by FCM (Fuzzy C-means). Accordingly, they are then transferred into HSV color space, and the optimal matched results would be obtained by minimizing the weighted distance of the H and S channels. Eventually, the original image would be significantly enhanced using our improved color transfer algorithm. Therefore, this novel low-light image enhancement algorithm based on information assistance expands the application of color transfer in the field of low-light image enhancement. In conclusion, the proposed algorithm has made superior achievements in image contrast retention, color consistency, and naturalness preservation, which would be promising in broad application scenarios.",
    "url": "https://ieeexplore.ieee.org/document/9956275",
    "tags": [
      "Low-light Image Enhancement"
    ],
    "homepage": "https://srameo.github.io/bibtex/guo2022novel.html",
    "published": "ICPR 2022",
    "teaser": "<img src='/assets/paper_teaser/icpr2023teaser.png' style='width: 99%; height: auto;'>",
    "core_contributor": false,
    "pos": [
      3.166100025177002,
      8.713299751281738,
      -0.420199990272522
    ],
    "stars": 0
  },
  {
    "id": "guo2023underwater",
    "title": "Underwater Ranker: Learn Which Is Better and How to Be Better",
    "authors": [
      "Chunle Guo",
      "Ruiqi Wu",
      "Xin Jin",
      "Linghao Han",
      "Zhi Chai",
      "Weidong Zhang",
      "Chongyi Li"
    ],
    "abstract": "In this paper, we present a ranking-based underwater image quality assessment (UIQA) method, abbreviated as URanker. The URanker is built on the efficient conv-attentional image Transformer. In terms of underwater images, we specially devise (1) the histogram prior that embeds the color distribution of an underwater image as histogram token to attend global degradation and (2) the dynamic cross-scale correspondence to model local degradation. The final prediction depends on the class tokens from different scales, which comprehensively considers multi-scale dependencies. With the margin ranking loss, our URanker can accurately rank the order of underwater images of the same scene enhanced by different underwater image enhancement (UIE) algorithms according to their visual quality. To achieve that, we also contribute a dataset, URankerSet, containing sufficient results enhanced by different UIE algorithms and the corresponding perceptual rankings, to train our URanker. Apart from the good performance of URanker, we found that a simple U-shape UIE network can obtain promising performance when it is coupled with our pre-trained URanker as additional supervision. In addition, we also propose a normalization tail that can significantly improve the performance of UIE networks. Extensive experiments demonstrate the state-of-the-art performance of our method. The key designs of our method are discussed.",
    "url": "https://arxiv.org/abs/2208.06857",
    "tags": [
      "Underwater"
    ],
    "homepage": "https://li-chongyi.github.io/URanker_files/",
    "published": "AAAI 2023",
    "teaser": "<img src='/assets/paper_teaser/uranker_teaser.png' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/RQ-Wu/UnderwaterRanker",
    "core_contributor": false,
    "pos": [
      2.790600061416626,
      10.222700119018555,
      0.37459999322891235
    ],
    "stars": 118
  },
  {
    "id": "jin2023dnf",
    "title": "DNF: Decouple and Feedback Network for Seeing in the Dark",
    "authors": [
      "Xin Jin",
      "Linghao Han",
      "Zhen Li",
      "Zhi Chai",
      "Chunle Guo",
      "Chongyi Li"
    ],
    "abstract": "The exclusive properties of RAW data have shown great potential for low-light image enhancement. Nevertheless, the performance is bottlenecked by the inherent limitations of existing architectures in both single-stage and multi-stage methods. Mixed mapping across two different domains, noise-to-clean and RAW-to-sRGB, misleads the single-stage methods due to the domain ambiguity. The multi-stage methods propagate the information merely through the resulting image of each stage, neglecting the abundant features in the lossy image-level dataflow. In this paper, we probe a generalized solution to these bottlenecks and propose a Decouple aNd Feedback framework, abbreviated as DNF. To mitigate the domain ambiguity, domain-specific subtasks are decoupled, along with fully utilizing the unique properties in RAW and sRGB domains. The feature propagation across stages with a feedback mechanism avoids the information loss caused by image-level dataflow. The two key insights of our method resolve the inherent limitations of RAW data-based low-light image enhancement satisfactorily, empowering our method to outperform the previous state-of-the-art method by a large margin with only 19% parameters, achieving 0.97dB and 1.30dB PSNR improvements on the Sony and Fuji subsets of SID.",
    "url": "https://ieeexplore.ieee.org/document/10204662",
    "tags": [
      "Camera",
      "ISP"
    ],
    "homepage": "https://github.com/Srameo/DNF",
    "published": "CVPR 2023",
    "teaser": "<img src='/assets/paper_teaser/dnf_teaser.gif' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Srameo/DNF",
    "core_contributor": true,
    "pos": [
      2.455199956893921,
      9.025400161743164,
      -0.4564000070095062
    ],
    "stars": 174
  },
  {
    "id": "jin2023lighting",
    "title": "Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising",
    "authors": [
      "Xin Jin",
      "Jia-Wen Xiao",
      "Linghao Han",
      "Chunle Guo",
      "Ruixun Zhang",
      "Xialei Liu",
      "Chongyi Li"
    ],
    "abstract": "Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods.",
    "url": "https://arxiv.org/abs/2308.03448v1",
    "tags": [
      "Camera",
      "ISP"
    ],
    "homepage": "https://srameo.github.io/projects/led-iccv23/",
    "published": "ICCV 2023",
    "teaser": "<img src='/assets/paper_teaser/led_teaser.png' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Srameo/LED",
    "core_contributor": true,
    "pos": [
      2.162899971008301,
      8.539199829101562,
      0.16369999945163727
    ],
    "stars": 389
  },
  {
    "id": "jin2023make",
    "title": "Make Explict Calibration Implicit: \"Calibrate\" Denoiser Instead of The Noise Model",
    "authors": [
      "Xin Jin",
      "Jia-Wen Xiao",
      "Linghao Han",
      "Chunle Guo",
      "Xialei Liu",
      "Chongyi Li",
      "Ming-Ming Cheng"
    ],
    "abstract": "Explicit calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods are impeded by several critical limitations: a) the explicit calibration process is both labor- and time-intensive, b) challenge exists in transferring denoisers across different camera models, and c) the disparity between synthetic and real noise is exacerbated by digital gain. To address these issues, we introduce a groundbreaking pipeline named Lighting Every Darkness (LED), which is effective regardless of the digital gain or the camera sensor. LED eliminates the need for explicit noise model calibration, instead utilizing an implicit fine-tuning process that allows quick deployment and requires minimal data. Structural modifications are also included to reduce the discrepancy between synthetic and real noise without extra computational demands. Our method surpasses existing methods in various camera models, including new ones not in public datasets, with just a few pairs per digital gain and only 0.5% of the typical iterations. Furthermore, LED also allows researchers to focus more on deep learning advancements while still utilizing sensor engineering benefits.",
    "url": "https://arxiv.org/abs/2308.03448v1",
    "tags": [
      "Camera",
      "ISP"
    ],
    "homepage": "https://srameo.github.io/projects/led-extension/",
    "published": "Arxiv Preprint",
    "teaser": "<img src='/projects/led-extension/assets/multiraw.jpg' style='width: 99%; height: auto;'>",
    "core_contributor": true,
    "pos": [
      3.2625999450683594,
      9.30270004272461,
      0.15860000252723694
    ],
    "stars": 0
  },
  {
    "id": "jin2024le3d",
    "title": "Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis",
    "authors": [
      "Xin Jin",
      "Pengyi Jiao",
      "Zheng-Peng Duan",
      "Xingchao Yang",
      "Chun-Le Guo",
      "Bo Ren",
      "Chong-Yi Li"
    ],
    "abstract": "Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes. While, they suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in distant views; 2) the limited representation capacity of spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM, and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for downstream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous volumetric rendering based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS.",
    "url": "https://arxiv.org/abs/2411.15678",
    "tags": [
      "Camera",
      "3D Reconstruction",
      "ISP",
      "HDR"
    ],
    "homepage": "https://srameo.github.io/projects/le3d/",
    "published": "NeurIPS 2024",
    "teaser": "<img src='/assets/paper_teaser/le3d.gif' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Srameo/LE3D",
    "core_contributor": true,
    "pos": [
      3.4377999305725098,
      8.53219985961914,
      0.6412000060081482
    ],
    "stars": 232
  },
  {
    "id": "jin2025classic",
    "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
    "authors": [
      "Xin Jin",
      "Simon Niklaus",
      "Zhoutong Zhang",
      "Zhihao Xia",
      "Chunle Guo",
      "Yuting Yang",
      "Jiawen Chen",
      "Chong-Yi Li"
    ],
    "abstract": "Denoising is a crucial step in many video processing pipelines such as in interactive editing, where high quality, speed, and user control are essential. While recent approaches achieve significant improvements in denoising quality by leveraging deep learning, they are prone to unexpected failures due to discrepancies between training data distributions and the wide variety of noise patterns found in real-world videos. These methods also tend to be slow and lack user control. In contrast, traditional denoising methods perform reliably on in-the-wild videos and run relatively quickly on modern hardware. However, they require manually tuning parameters for each input video, which is not only tedious but also requires skill. We bridge the gap between these two paradigms by proposing a differentiable denoising pipeline based on traditional methods. A neural network is then trained to predict the optimal denoising parameters for each specific input, resulting in a robust and efficient approach that also supports user control.",
    "url": "https://arxiv.org/abs/2504.03136",
    "tags": [
      "Camera",
      "Video Denoising"
    ],
    "homepage": "https://srameo.github.io/projects/levd/",
    "published": "CVPR 2025",
    "teaser": "<img src='/assets/paper_teaser/levd.gif' style='width: 99%; height: auto;'>",
    "core_contributor": true,
    "pos": [
      2.196000099182129,
      9.370499610900879,
      0.36090001463890076
    ],
    "stars": 0
  },
  {
    "id": "li2024towards",
    "title": "Towards RAW Object Detection in Diverse Conditions",
    "authors": [
      "Zhong-Yu Li",
      "Xin Jin",
      "Boyuan Sun",
      "Chun-Le Guo",
      "Ming-Ming Cheng"
    ],
    "abstract": "Existing object detection methods often consider sRGB input, which was compressed from RAW data using ISP originally designed for visualization. However, such compression might lose crucial information for detection, especially under complex light and weather conditions. We introduce the AODRaw dataset, which offers 7,785 high-resolution real RAW images with 135,601 annotated instances spanning 62 categories, capturing a broad range of indoor and outdoor scenes under 9 distinct light and weather conditions. Based on AODRaw that supports RAW and sRGB object detection, we provide a comprehensive benchmark for evaluating current detection methods. We find that sRGB pre-training constrains the potential of RAW object detection due to the domain gap between sRGB and RAW, prompting us to directly pre-train on the RAW domain. However, it is harder for RAW pre-training to learn rich representations than sRGB pre-training due to the camera noise. To assist RAW pre-training, we distill the knowledge from an off-the-shelf model pre-trained on the sRGB domain. As a result, we achieve substantial improvements under diverse and adverse conditions without relying on extra pre-processing modules.",
    "url": "https://arxiv.org/abs/2411.15678",
    "tags": [
      "Camera",
      "Object Detection"
    ],
    "homepage": "https://github.com/lzyhha/AODRaw/",
    "published": "CVPR 2025",
    "teaser": "<img src='/assets/paper_teaser/aodraw_teaser.jpg' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/lzyhha/AODRaw",
    "core_contributor": false,
    "pos": [
      2.942199945449829,
      9.21030044555664,
      0.9815999865531921
    ],
    "stars": 36
  },
  {
    "id": "meng2025ultraled",
    "title": "Learning to See Everything in Ultra-High Dynamic Range Scenes",
    "authors": [
      "Yuang Meng",
      "Xin Jin",
      "Lina Lei",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "Ultra-high dynamic range (UHDR) scenes exhibit pronounced exposure disparities between bright and dark regions. Such conditions are common in nighttime scenes with light sources. Even standard exposure settings often result in a bimodal intensity distribution with boundary peaks, making it challenging to simultaneously preserve both highlight and shadow details. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. A short-exposure image, however, already retains sufficient highlight detail. The main challenge lies in denoising and recovering information in dark regions. RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? Our method, relying solely on one short-exposure frame, inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce a two-stage framework: exposure correction via a ratio map to balance dynamic range, followed by brightness-aware noise modeling to enhance detail recovery in dark regions. To support this, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images, and construct a dataset accordingly on static scenes, using only the shortest exposure as input for reconstruction. Experiments show that our method significantly outperforms existing single-frame approaches. Code will be released publicly.",
    "tags": [
      "Camera",
      "ISP",
      "HDR"
    ],
    "homepage": "https://srameo.github.io/projects/ultraled/",
    "published": "Arxiv Preprint",
    "teaser": "<img src='/assets/paper_teaser/ultraled.gif' style='width: 99%; height: auto;'>",
    "core_contributor": true,
    "pos": [
      2.808799982070923,
      8.469599723815918,
      0.3912999927997589
    ],
    "stars": 0
  },
  {
    "id": "liu2025decoupled",
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "authors": [
      "Dongyang Liu",
      "Peng Gao",
      "David Liu",
      "Ruoyi Du",
      "Zhen Li",
      "Qilong Wu",
      "Xin Jin",
      "Sihan Cao",
      "Shifeng Zhang",
      "Hongsheng Li",
      "Steven HOI"
    ],
    "abstract": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student’s output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFGis typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core “engine” of distillation, while the Distribution Matching (DM) term functions as a “regularizer” that ensures training stability and mitigates artifacts. Wefurther validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
    "url": "https://www.arxiv.org/abs/2511.22677",
    "tags": [
      "Diffusion",
      "Distillation"
    ],
    "homepage": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
    "published": "Arxiv Preprint",
    "teaser": "<img src='/assets/paper_teaser/decoupled_dmd.jpg' style='width: 99%; height: auto;'>",
    "core_contributor": false,
    "pos": [
      2.2402000427246094,
      10.572400093078613,
      1.367799997329712
    ],
    "stars": 0
  },
  {
    "id": "jiang2025dmdrl",
    "title": "Distribution Matching Distillation Meets Reinforcement Learning",
    "authors": [
      "Dengyang Jiang",
      "Dongyang Liu",
      "Zanyi Wang",
      "Qilong Wu",
      "Liuzhuozheng Li",
      "Hengzhuang Li",
      "Xin Jin",
      "David Liu",
      "Zhen Li",
      "Bo Zhang",
      "Mengmeng Wang",
      "Steven Hoi",
      "Peng Gao",
      "Harry Yang"
    ],
    "abstract": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
    "url": "https://arxiv.org/abs/2511.13649",
    "tags": [
      "Diffusion",
      "Reinforcement Learning"
    ],
    "homepage": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
    "published": "Arxiv Preprint",
    "teaser": "<img src='/assets/paper_teaser/dmd_rl.jpg' style='width: 99%; height: auto;'>",
    "core_contributor": false,
    "pos": [
      2.887700080871582,
      10.555500030517578,
      1.3375999927520752
    ],
    "stars": 0
  },
  {
    "id": "z-image",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "authors": [
      "Z-Image Team"
    ],
    "abstract": "The landscape of high-performance image generation models is currently dominated by pro-prietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-sourcealternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are charac-terized by massive parameter counts (20B to 80B), making them impractical for inference, andfine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle – from a curated data infrastructure to a streamlined training curriculum – we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealisticimage generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
    "url": "https://arxiv.org/abs/2511.22699",
    "tags": [
      "Diffusion",
      "Base Model"
    ],
    "homepage": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
    "published": "Tech Report",
    "teaser": "<img src='/assets/paper_teaser/s3_dit.png' style='width: 99%; height: auto;'>",
    "github_url": "https://github.com/Tongyi-MAI/Z-Image",
    "core_contributor": true,
    "pos": [
      2.5966999530792236,
      9.92199993133545,
      1.642199993133545
    ],
    "stars": 8988
  }
]